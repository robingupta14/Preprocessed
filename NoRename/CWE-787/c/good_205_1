







DEFINE_PER_CPU_ALIGNED(irq_cpustat_t, irq_stat);
EXPORT_PER_CPU_SYMBOL(irq_stat);


static struct softirq_action softirq_vec[NR_SOFTIRQS] __cacheline_aligned_in_smp;

DEFINE_PER_CPU(struct task_struct *, ksoftirqd);

const char * const softirq_to_name[NR_SOFTIRQS] = {
 "HI", "TIMER", "NET_TX", "NET_RX", "BLOCK", "IRQ_POLL",
 "TASKLET", "SCHED", "HRTIMER", "RCU"
};







static void wakeup_softirqd(void)
{

 struct task_struct *tsk = __this_cpu_read(ksoftirqd);

 if (tsk && tsk->state != TASK_RUNNING)
  wake_up_process(tsk);
}





static bool ksoftirqd_running(void)
{
 struct task_struct *tsk = __this_cpu_read(ksoftirqd);

 return tsk && (tsk->state == TASK_RUNNING);
}

static void __local_bh_enable(unsigned int cnt)
{
 lockdep_assert_irqs_disabled();

 if (preempt_count() == cnt)
  trace_preempt_on(CALLER_ADDR0, get_lock_parent_ip());

 if (softirq_count() == (cnt & SOFTIRQ_MASK))
  trace_softirqs_on(_RET_IP_);

 __preempt_count_sub(cnt);
}





void _local_bh_enable(void)
{
 WARN_ON_ONCE(in_irq());
 __local_bh_enable(SOFTIRQ_DISABLE_OFFSET);
}
EXPORT_SYMBOL(_local_bh_enable);

void __local_bh_enable_ip(unsigned long ip, unsigned int cnt)
{
 WARN_ON_ONCE(in_irq());
 lockdep_assert_irqs_enabled();






 if (softirq_count() == SOFTIRQ_DISABLE_OFFSET)
  trace_softirqs_on(ip);




 preempt_count_sub(cnt - 1);

 if (unlikely(!in_interrupt() && local_softirq_pending())) {




  do_softirq();
 }

 preempt_count_dec();



 preempt_check_resched();
}
EXPORT_SYMBOL(__local_bh_enable_ip);

static inline bool lockdep_softirq_start(void) { return false; }
static inline void lockdep_softirq_end(bool in_hardirq) { }


asmlinkage __visible void __softirq_entry __do_softirq(void)
{
 unsigned long end = jiffies + msecs_to_jiffies(2);
 unsigned long old_flags = current->flags;
 int max_restart = 10;
 struct softirq_action *h;
 bool in_hardirq;
 __u32 pending;
 int softirq_bit;






 current->flags &= ~PF_MEMALLOC;

 pending = local_softirq_pending();
 account_irq_enter_time(current);

 __local_bh_disable_ip(_RET_IP_, SOFTIRQ_OFFSET);
 in_hardirq = lockdep_softirq_start();

restart:

 set_softirq_pending(0);

 local_irq_enable();

 h = softirq_vec;

 while ((softirq_bit = ffs(pending))) {
  unsigned int vec_nr;
  int prev_count;

  h += softirq_bit - 1;

  vec_nr = h - softirq_vec;
  prev_count = preempt_count();

  kstat_incr_softirqs_this_cpu(vec_nr);

  trace_softirq_entry(vec_nr);
  h->action(h);
  trace_softirq_exit(vec_nr);
  if (unlikely(prev_count != preempt_count())) {
   pr_err("huh, entered softirq %u %s %p with preempt_count %08x, exited with %08x?\n",
          vec_nr, softirq_to_name[vec_nr], h->action,
          prev_count, preempt_count());
   preempt_count_set(prev_count);
  }
  h++;
  pending >>= softirq_bit;
 }

 rcu_bh_qs();
 local_irq_disable();

 pending = local_softirq_pending();
 if (pending) {
  if (time_before(jiffies, end) && !need_resched() &&
      --max_restart)
   goto restart;

  wakeup_softirqd();
 }

 lockdep_softirq_end(in_hardirq);
 account_irq_exit_time(current);
 __local_bh_enable(SOFTIRQ_OFFSET);
 WARN_ON_ONCE(in_interrupt());
 current_restore_flags(old_flags, PF_MEMALLOC);
}

asmlinkage __visible void do_softirq(void)
{
 __u32 pending;
 unsigned long flags;

 if (in_interrupt())
  return;

 local_irq_save(flags);

 pending = local_softirq_pending();

 if (pending && !ksoftirqd_running())
  do_softirq_own_stack();

 local_irq_restore(flags);
}




void irq_enter(void)
{
 rcu_irq_enter();
 if (is_idle_task(current) && !in_interrupt()) {




  local_bh_disable();
  tick_irq_enter();
  _local_bh_enable();
 }

 __irq_enter();
}

static inline void invoke_softirq(void)
{
 if (ksoftirqd_running())
  return;

 if (!force_irqthreads) {

  do_softirq_own_stack();

 } else {
  wakeup_softirqd();
 }
}

static inline void tick_irq_exit(void)
{

}




void irq_exit(void)
{

 local_irq_disable();



 account_irq_exit_time(current);
 preempt_count_sub(HARDIRQ_OFFSET);
 if (!in_interrupt() && local_softirq_pending())
  invoke_softirq();

 tick_irq_exit();
 rcu_irq_exit();
 trace_hardirq_exit();
}




inline void raise_softirq_irqoff(unsigned int nr)
{
 __raise_softirq_irqoff(nr);

 if (!in_interrupt())
  wakeup_softirqd();
}

void raise_softirq(unsigned int nr)
{
 unsigned long flags;

 local_irq_save(flags);
 raise_softirq_irqoff(nr);
 local_irq_restore(flags);
}

void __raise_softirq_irqoff(unsigned int nr)
{
 trace_softirq_raise(nr);
 or_softirq_pending(1UL << nr);
}

void open_softirq(int nr, void (*action)(struct softirq_action *))
{
 softirq_vec[nr].action = action;
}




struct tasklet_head {
 struct tasklet_struct *head;
 struct tasklet_struct **tail;
};

static DEFINE_PER_CPU(struct tasklet_head, tasklet_vec);
static DEFINE_PER_CPU(struct tasklet_head, tasklet_hi_vec);

static void __tasklet_schedule_common(struct tasklet_struct *t,
          struct tasklet_head __percpu *headp,
          unsigned int softirq_nr)
{
 struct tasklet_head *head;
 unsigned long flags;

 local_irq_save(flags);
 head = this_cpu_ptr(headp);
 t->next = NULL;
 *head->tail = t;
 head->tail = &(t->next);
 raise_softirq_irqoff(softirq_nr);
 local_irq_restore(flags);
}

void __tasklet_schedule(struct tasklet_struct *t)
{
 __tasklet_schedule_common(t, &tasklet_vec,
      TASKLET_SOFTIRQ);
}
EXPORT_SYMBOL(__tasklet_schedule);

void __tasklet_hi_schedule(struct tasklet_struct *t)
{
 __tasklet_schedule_common(t, &tasklet_hi_vec,
      HI_SOFTIRQ);
}
EXPORT_SYMBOL(__tasklet_hi_schedule);

static void tasklet_action_common(struct softirq_action *a,
      struct tasklet_head *tl_head,
      unsigned int softirq_nr)
{
 struct tasklet_struct *list;

 local_irq_disable();
 list = tl_head->head;
 tl_head->head = NULL;
 tl_head->tail = &tl_head->head;
 local_irq_enable();

 while (list) {
  struct tasklet_struct *t = list;

  list = list->next;

  if (tasklet_trylock(t)) {
   if (!atomic_read(&t->count)) {
    if (!test_and_clear_bit(TASKLET_STATE_SCHED,
       &t->state))
     BUG();
    t->func(t->data);
    tasklet_unlock(t);
    continue;
   }
   tasklet_unlock(t);
  }

  local_irq_disable();
  t->next = NULL;
  *tl_head->tail = t;
  tl_head->tail = &t->next;
  __raise_softirq_irqoff(softirq_nr);
  local_irq_enable();
 }
}

static __latent_entropy void tasklet_action(struct softirq_action *a)
{
 tasklet_action_common(a, this_cpu_ptr(&tasklet_vec), TASKLET_SOFTIRQ);
}

static __latent_entropy void tasklet_hi_action(struct softirq_action *a)
{
 tasklet_action_common(a, this_cpu_ptr(&tasklet_hi_vec), HI_SOFTIRQ);
}

void tasklet_init(struct tasklet_struct *t,
    void (*func)(unsigned long), unsigned long data)
{
 t->next = NULL;
 t->state = 0;
 atomic_set(&t->count, 0);
 t->func = func;
 t->data = data;
}
EXPORT_SYMBOL(tasklet_init);

void tasklet_kill(struct tasklet_struct *t)
{
 if (in_interrupt())
  pr_notice("Attempt to kill tasklet from interrupt\n");

 while (test_and_set_bit(TASKLET_STATE_SCHED, &t->state)) {
  do {
   yield();
  } while (test_bit(TASKLET_STATE_SCHED, &t->state));
 }
 tasklet_unlock_wait(t);
 clear_bit(TASKLET_STATE_SCHED, &t->state);
}
EXPORT_SYMBOL(tasklet_kill);

static enum hrtimer_restart __hrtimer_tasklet_trampoline(struct hrtimer *timer)
{
 struct tasklet_hrtimer *ttimer =
  container_of(timer, struct tasklet_hrtimer, timer);

 tasklet_hi_schedule(&ttimer->tasklet);
 return HRTIMER_NORESTART;
}





static void __tasklet_hrtimer_trampoline(unsigned long data)
{
 struct tasklet_hrtimer *ttimer = (void *)data;
 enum hrtimer_restart restart;

 restart = ttimer->function(&ttimer->timer);
 if (restart != HRTIMER_NORESTART)
  hrtimer_restart(&ttimer->timer);
}

void tasklet_hrtimer_init(struct tasklet_hrtimer *ttimer,
     enum hrtimer_restart (*function)(struct hrtimer *),
     clockid_t which_clock, enum hrtimer_mode mode)
{
 hrtimer_init(&ttimer->timer, which_clock, mode);
 ttimer->timer.function = __hrtimer_tasklet_trampoline;
 tasklet_init(&ttimer->tasklet, __tasklet_hrtimer_trampoline,
       (unsigned long)ttimer);
 ttimer->function = function;
}
EXPORT_SYMBOL_GPL(tasklet_hrtimer_init);

void __init softirq_init(void)
{
 int cpu;

 for_each_possible_cpu(cpu) {
  per_cpu(tasklet_vec, cpu).tail =
   &per_cpu(tasklet_vec, cpu).head;
  per_cpu(tasklet_hi_vec, cpu).tail =
   &per_cpu(tasklet_hi_vec, cpu).head;
 }

 open_softirq(TASKLET_SOFTIRQ, tasklet_action);
 open_softirq(HI_SOFTIRQ, tasklet_hi_action);
}

static int ksoftirqd_should_run(unsigned int cpu)
{
 return local_softirq_pending();
}

static void run_ksoftirqd(unsigned int cpu)
{
 local_irq_disable();
 if (local_softirq_pending()) {




  __do_softirq();
  local_irq_enable();
  cond_resched();
  return;
 }
 local_irq_enable();
}

static struct smp_hotplug_thread softirq_threads = {
 .store = &ksoftirqd,
 .thread_should_run = ksoftirqd_should_run,
 .thread_fn = run_ksoftirqd,
 .thread_comm = "ksoftirqd/%u",
};

static __init int spawn_ksoftirqd(void)
{
 cpuhp_setup_state_nocalls(CPUHP_SOFTIRQ_DEAD, "softirq:dead", NULL,
      NULL);
 BUG_ON(smpboot_register_percpu_thread(&softirq_threads));

 return 0;
}
early_initcall(spawn_ksoftirqd);






int __init __weak early_irq_init(void)
{
 return 0;
}

int __init __weak arch_probe_nr_irqs(void)
{
 return NR_IRQS_LEGACY;
}

int __init __weak arch_early_irq_init(void)
{
 return 0;
}

unsigned int __weak arch_dynirq_lower_bound(unsigned int from)
{
 return from;
}
